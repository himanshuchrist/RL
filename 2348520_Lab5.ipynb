{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "85f9466c-31fb-41ab-bd40-3b0f4ffe36ed",
   "metadata": {},
   "source": [
    "## Model-Free Prediction & Control With Monte Carlo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d1b6c13-8568-486e-ba0e-2fffdd7bb59b",
   "metadata": {},
   "source": [
    "### Initialisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "94df79a2-35cd-43ca-a963-8327dab51d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "grid = np.array([\n",
    "    [0, 0, 0, 1],\n",
    "    [0, 3, 0, 2],\n",
    "    [0, 0, 0, 0]\n",
    "])\n",
    "rewards = {0: -0.04, 1: 1, 2: -1, 3: 0}\n",
    "actions = ['up', 'down', 'left', 'right']\n",
    "action_effects = {\n",
    "    'up': (-1, 0),\n",
    "    'down': (1, 0),\n",
    "    'left': (0, -1),\n",
    "    'right': (0, 1)\n",
    "}\n",
    "discount_factor = 0.9\n",
    "epsilon = 0.1 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede54e9d-4a57-4d27-aad3-599d6daea875",
   "metadata": {},
   "source": [
    "### Check for terminal state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "68ff09a6-dfb7-43db-8413-854eac09ad60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_terminal(state):\n",
    "    i, j = state\n",
    "    return grid[i, j] in (1, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c311a5-3492-4f92-92d2-6e896862d245",
   "metadata": {},
   "source": [
    "### Next State "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f7824bea-471b-46b1-aae0-200ac5347f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def next(state, action):\n",
    "    i, j = state\n",
    "    di, dj = action_effects[action]\n",
    "    next_i, next_j = i + di, j + dj\n",
    "    \n",
    "    if 0 <= next_i < grid.shape[0] and 0 <= next_j < grid.shape[1]:\n",
    "        return next_i, next_j\n",
    "    return state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43ae7970-c355-46ad-b32c-15ca5eade7a6",
   "metadata": {},
   "source": [
    "### Îµ-greedy policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "68856bfe-e25b-4c7a-8a50-1ad5227c8700",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def epsilon_greedy_policy(Q, state):\n",
    "    if random.random() < epsilon:\n",
    "        return random.choice(actions) \n",
    "    else:\n",
    "        return max(actions, key=lambda a: Q[state, a])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb160962-62a5-4dc7-803e-ddb1c2f55965",
   "metadata": {},
   "source": [
    "### Finding optimal Policy using using MC control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f44056e7-3989-4542-8a66-5f7b9000343c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "def monte_carlo_control(num_episodes=5000):\n",
    "    Q = defaultdict(float) \n",
    "    returns = defaultdict(list) \n",
    "    policy = {}\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        state = (2, 0) \n",
    "        episode = []\n",
    "        \n",
    "        while not is_terminal(state):\n",
    "            action = epsilon_greedy_policy(Q, state)\n",
    "            next_state = next(state, action)\n",
    "            reward = rewards[grid[state]]\n",
    "            episode.append((state, action, reward))\n",
    "            state = next_state\n",
    "        \n",
    "        G = 0  \n",
    "        visited = set()  \n",
    "        for t in reversed(range(len(episode))):\n",
    "            state, action, reward = episode[t]\n",
    "            G = reward + discount_factor * G\n",
    "            if (state, action) not in visited:\n",
    "                visited.add((state, action))\n",
    "                returns[(state, action)].append(G)\n",
    "                Q[state, action] = np.mean(returns[(state, action)])\n",
    "    \n",
    "    for state_action in Q.keys():\n",
    "        state, action = state_action\n",
    "        if state not in policy:\n",
    "            policy[state] = max(actions, key=lambda a: Q[state, a])\n",
    "    \n",
    "    return Q, policy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6b08f9a2-85bb-4a0c-9b77-e37f06983a34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learned Q-values:\n",
      "State (2, 0) Action up: -0.12\n",
      "State (2, 0) Action down: -0.15\n",
      "State (2, 0) Action left: -0.15\n",
      "State (2, 0) Action right: -0.12\n",
      "State (1, 0) Action up: -0.21\n",
      "State (1, 0) Action down: -0.19\n",
      "State (1, 0) Action left: -0.19\n",
      "State (1, 0) Action right: -0.08\n",
      "State (0, 0) Action up: -0.37\n",
      "State (0, 0) Action down: -0.37\n",
      "State (0, 0) Action left: -0.30\n",
      "State (0, 0) Action right: -0.16\n",
      "State (0, 2) Action up: -0.08\n",
      "State (0, 2) Action down: -0.16\n",
      "State (0, 2) Action left: -0.15\n",
      "State (0, 2) Action right: -0.04\n",
      "State (1, 2) Action right: -0.04\n",
      "State (0, 1) Action right: -0.08\n",
      "State (2, 1) Action up: -0.08\n",
      "State (2, 1) Action down: -0.12\n",
      "State (2, 1) Action left: -0.15\n",
      "State (2, 1) Action right: -0.11\n",
      "State (1, 1) Action up: -0.08\n",
      "State (1, 1) Action down: -0.08\n",
      "State (1, 1) Action left: -0.08\n",
      "State (1, 1) Action right: -0.04\n",
      "State (0, 1) Action up: -0.11\n",
      "State (0, 1) Action down: -0.13\n",
      "State (0, 1) Action left: -0.23\n",
      "State (1, 2) Action up: -0.08\n",
      "State (1, 2) Action down: -0.12\n",
      "State (1, 2) Action left: -0.08\n",
      "State (2, 2) Action up: -0.09\n",
      "State (2, 2) Action down: -0.11\n",
      "State (2, 2) Action left: -0.14\n",
      "State (2, 2) Action right: -0.08\n",
      "State (2, 3) Action up: -0.04\n",
      "State (2, 3) Action down: -0.08\n",
      "State (2, 3) Action left: -0.11\n",
      "State (2, 3) Action right: -0.08\n"
     ]
    }
   ],
   "source": [
    "Q, policy = monte_carlo_control()\n",
    "\n",
    "print(\"Learned Q-values:\")\n",
    "for key, value in Q.items():\n",
    "    print(f\"State {key[0]} Action {key[1]}: {value:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c574ba0b-9682-4639-b179-202807e3f663",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Derived Policy:\n",
      "State (2, 0): right\n",
      "State (1, 0): right\n",
      "State (0, 0): right\n",
      "State (0, 2): right\n",
      "State (1, 2): right\n",
      "State (0, 1): right\n",
      "State (2, 1): up\n",
      "State (1, 1): right\n",
      "State (2, 2): right\n",
      "State (2, 3): up\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nDerived Policy:\")\n",
    "for state, action in policy.items():\n",
    "    print(f\"State {state}: {action}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06fba385-3701-4940-beeb-fbdb4e0dd1f3",
   "metadata": {},
   "source": [
    "## END"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
